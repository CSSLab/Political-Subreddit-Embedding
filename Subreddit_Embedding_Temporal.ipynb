{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/h/224/cameron/spark-3.0.0-preview2-bin-hadoop2.7\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_sub\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()\n",
    "TIME_FRAME = \"monthly\"\n",
    "window = {\n",
    "    \"weekly\": 7,\n",
    "    \"biweekly\": 14,\n",
    "    \"monthly\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = false)\n",
      " |-- subreddit: string (nullable = false)\n",
      " |-- created_utc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Parquet data\n",
    "comments = spark.read.load(\"/comments_2019.parquet\").fillna(\"\")\n",
    "subreddits = spark.read.load(\"dataframes/subreddits.parquet\")\n",
    "comments = comments.join(subreddits, ['subreddit'], 'leftsemi')\n",
    "cols = ['author','subreddit','created_utc']\n",
    "comments = comments.select(*cols)\n",
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|          author|           subreddit|\n",
      "+----------------+--------------------+\n",
      "|     jncummins86|(stilltrying,1/3/...|\n",
      "|     andrewmyles|(NintendoSwitch,1...|\n",
      "|           -QBM-|(LofiHipHop,1/3/2...|\n",
      "|    BlazeGiraffe|  (cocaine,1/3/2019)|\n",
      "|         SeeDeez|   (nyjets,1/3/2019)|\n",
      "|   Serious_Sam_2|(RocketLeagueExch...|\n",
      "|       [deleted]|(moist_memes,1/3/...|\n",
      "|       [deleted]|(worldnews,1/3/2019)|\n",
      "|IrishEyesRsmilin|(StevenAveryIsGui...|\n",
      "|  RanietsSharvas|(AnthemTheGame,1/...|\n",
      "|          aeolid|(Sourdough,1/3/2019)|\n",
      "|        jobn2021|(WaltDisneyWorld,...|\n",
      "|  KillUrselfAcne|(8BallPool,1/3/2019)|\n",
      "|       Bankertov|(StreetFighter,1/...|\n",
      "|       HiImDavid|(guineapigs,1/3/2...|\n",
      "|        Nole2424|(apexlegends,1/3/...|\n",
      "|      1000Dragon|(Maplestory,1/3/2...|\n",
      "|     mrfuckhead1|(synthesizers,1/3...|\n",
      "|        phcullen|(Charlotte,1/3/2019)|\n",
      "|   thelucidvegan|   (movies,1/3/2019)|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add date column\n",
    "# Create a function that returns the desired UDF from a timestamp \n",
    "to_udf = udf(lambda ts: datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:00:00\"))\n",
    "\n",
    "comments = comments.withColumn(\"timestamp\", to_udf(comments[\"created_utc\"]))\n",
    "if TIME_FRAME in [\"weekly\",\"biweekly\"]:\n",
    "    # Add column that aggregates by week \n",
    "    comments = comments.withColumn(\"week\",date_sub(next_day(col(\"timestamp\"),\"sunday\"),window[TIME_FRAME]))\n",
    "    comments = comments.withColumn('subreddit', concat(lit('('),col('subreddit'),lit(','),col('week'),lit(')')))\n",
    "    comments = comments.drop(*[\"created_utc\",\"timestamp\",\"week\"])\n",
    "else:\n",
    "    comments = comments.withColumn('month', date_format(col('timestamp'), '1/M/yyyy'))\n",
    "    comments = comments.withColumn('subreddit', concat(lit('('),col('subreddit'),lit(','),col('month'),lit(')')))\n",
    "    comments = comments.drop(*[\"created_utc\",\"timestamp\",\"month\"])\n",
    "\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecf Files\n",
    "[Word2vecf](https://github.com/BIU-NLP/word2vecf/blob/master/README.md) requires three inputs\n",
    "* training_data: text file of word-context pairs (space delimited)\n",
    "* word_vocabulary: file mapping subreddits (strings) to their counts\n",
    "* count_vocabulary: file mapping users (contexts -> subreddit commenters) to their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "We want to avoid having to load the raw data as there are 1 billion+ rows. Working with aggregates from the start makes things much easier. \n",
    "\n",
    "*Since this is the temporal embedding we've already added the week into the subreddit name. Each subreddit/week combo is considered a new word with possible different contexts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|           subreddit|             author|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   (hockey,1/3/2019)|        Intylerable|   12|\n",
      "|(CanadaPublicServ...|        AntonBanton|    5|\n",
      "|(entitledparents,...|        viscool8332|  118|\n",
      "|(todayilearned,1/...|             hilti2|    4|\n",
      "|      (sex,1/3/2019)|         realistnic|    1|\n",
      "| (politics,1/3/2019)|       smikelsmikel|   93|\n",
      "|(elderscrollsonli...|   the_scarlet_ibis|   12|\n",
      "|(h3h3productions,...|              2dros|    1|\n",
      "|(AskReddit,1/3/2019)|     Levicorpyutani|    9|\n",
      "|(solotravel,1/3/2...|            EmmalNz|   93|\n",
      "|   (GoNets,1/3/2019)|         BlaackkOuT|  297|\n",
      "| (startrek,1/3/2019)|          rebbsitor|   13|\n",
      "|(legaladvice,1/3/...|          [deleted]|20628|\n",
      "|(microgrowery,1/3...|Ihavenobusinesshere|   12|\n",
      "|(StarWarsLeaks,1/...|      kingpenguinJG|   57|\n",
      "|(traaaaaaannnnnnn...|             Rota_u|   30|\n",
      "|(unpopularopinion...| YodasRedditAccount|  421|\n",
      "|(marvelstudios,1/...|             IsIt77|   87|\n",
      "|(AskReddit,1/3/2019)|          zeppeIans|   21|\n",
      "|(progmetal,1/3/2019)|          odichthys|   11|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = comments.groupBy([\"subreddit\",\"author\"]).count().cache()\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319777858"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|           subreddit|  count|\n",
      "+--------------------+-------+\n",
      "|(Allergies,1/3/2019)|   2138|\n",
      "|(FortNiteBR,1/3/2...| 652991|\n",
      "|(BuyItForLife,1/3...|  11572|\n",
      "|   (iphone,1/3/2019)|  53763|\n",
      "|(dankmemes,1/3/2019)|1303112|\n",
      "|   (Tgirls,1/3/2019)|   3693|\n",
      "|(DissidiaFFOO,1/3...|  27247|\n",
      "|     (cats,1/3/2019)|  94074|\n",
      "|   (AFROTC,1/3/2019)|   1423|\n",
      "|  (gaybros,1/3/2019)|  28645|\n",
      "|    (Jokes,1/3/2019)| 114812|\n",
      "|   (occult,1/3/2019)|  15956|\n",
      "|   (mexico,1/3/2019)|  52126|\n",
      "|     (bdsm,1/3/2019)|   5622|\n",
      "|(DeepIntoYouTube,...|   7981|\n",
      "|(NFL_Draft,1/3/2019)|  25266|\n",
      "|(wildhearthstone,...|   5615|\n",
      "|(Splatoon_2,1/3/2...|   3915|\n",
      "|(TaylorSwift,1/3/...|  14943|\n",
      "|     (army,1/3/2019)|  52576|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "word_vocabulary = training_data.groupBy(\"subreddit\").agg(_sum('count').alias('count')).cache()\n",
    "word_vocabulary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120825"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocabulary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|          author|count|\n",
      "+----------------+-----+\n",
      "|the_scarlet_ibis|   82|\n",
      "|       zeppeIans| 2118|\n",
      "|      SirDeVinci|  417|\n",
      "|      _Erindera_| 3913|\n",
      "| thundershocker1|  269|\n",
      "|          Sqiddd| 7489|\n",
      "|    weedwhacking| 2026|\n",
      "|    TeaTreeTeach|  613|\n",
      "|       oFaceless|  326|\n",
      "|  TheFirstUserID|  344|\n",
      "|         jingz13|   53|\n",
      "|     JustinBilyj|  400|\n",
      "|       deucemc26| 1996|\n",
      "|     tanngrisnit| 7821|\n",
      "|         hdv2017|  171|\n",
      "|    Mustache_Guy| 1841|\n",
      "|      uglygaming|  426|\n",
      "|        Embossis|   28|\n",
      "|     kevin123245|   66|\n",
      "|     SabrinaHiss|  110|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vocabulary = training_data.groupBy(\"author\").agg(_sum('count').alias('count')).cache()\n",
    "context_vocabulary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19338041"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocabulary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Vocabularies and Training Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='mkdir -p /h/224/cameron/Political-Subreddit-Embedding/temp/temporal/', returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temp context for the word and context vocabulary files (which get passed to the word2vecf script)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "temp_dir = \"/h/224/cameron/Political-Subreddit-Embedding/temp/temporal/\"\n",
    "subprocess.run(\"mkdir -p {}\".format(temp_dir), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp files\n",
    "file_data = os.path.join(temp_dir, '{}_data'.format(TIME_FRAME))\n",
    "file_wv = os.path.join(temp_dir, '{}_wv'.format(TIME_FRAME))\n",
    "file_cv = os.path.join(temp_dir, '{}_cv'.format(TIME_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training data to /h/224/cameron/Political-Subreddit-Embedding/temp/temporal/monthly_data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing training data to {}...\".format(file_data))\n",
    "training_data.write.csv(file_data,header=False,sep=' ')\n",
    "# training_data.toPandas().to_csv(file_data, header=False, index=False, sep=' ')\n",
    "# training_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing word vocab data to {}...\".format(file_wv))\n",
    "word_vocabulary.write.csv(file_wv,header=False,sep=' ')\n",
    "# word_vocabulary.toPandas().to_csv(file_wv, header=False, index=False, sep=' ')\n",
    "# word_vocabulary.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing context vocab data to {}...\".format(file_cv))\n",
    "context_vocabulary.write.csv(file_cv,header=False,sep=' ')\n",
    "# context_vocabulary.toPandas().to_csv(file_cv, header=False, index=False, sep=' ')\n",
    "# context_vocabulary.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import coalese_csvs\n",
    "file_data = coalese_csvs(file_data,\"{}.txt\".format(file_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_wv = coalese_csvs(file_data,\"{}.txt\".format(file_wv))\n",
    "file_cv = coalese_csvs(file_data,\"{}.txt\".format(file_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters, using negative sampling\n",
    "# -alpha 0.18 -negative 35 -sample 0.0043 -size 150\n",
    "from utils import generate_embedding, load_embedding\n",
    "embedding_args = {\n",
    "                    \"param1\": \"sample\", \n",
    "                    \"p1\": 0.0043, \n",
    "                    \"param2\": \"negative\", \n",
    "                    \"p2\": 35, \n",
    "                    \"file_data\": file_data , \n",
    "                    \"file_wv\": file_wv, \n",
    "                    \"file_cv\": file_cv,\n",
    "                    \"size\": 150,\n",
    "                    \"alpha\": 0.18\n",
    "                 }\n",
    "embedding = generate_embedding(embedding_args)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits, vectors = load_embedding(embedding)\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Out Subreddit from Week Again\n",
    "\n",
    "Since we've already trained all of the seperate emebeddings there isn't a need for them to be in the same column anymore. This will make animating the emebedding over time easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_tup\n",
    "\n",
    "sub_df = pd.DataFrame(subreddits.apply(parse_tup).tolist())\n",
    "sub_df.columns = [\"subreddit\",\"week\"]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize/Animate\n",
    "1. Reduce to 3/2 dimensions\n",
    "2. Add subreddit/week columns to factored dataframe\n",
    "3. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "subprocess.run(\"mkdir -p visualizations/temporal\", shell=True)\n",
    "left_subreddits = [\"JoeBiden\",\"Pete_Buttigieg\",\"Kamala\",\n",
    "                        \"SandersForPresident\",\"BetoORourke\",\"ElizabethWarren\",\n",
    "                        \"BaemyKlobaechar\",\"YangForPresidentHQ\",\"politics\",\"progressive\",\n",
    "                        \"demsocialist\",\"SocialDemocracy\",\"centerleftpolitics\",\n",
    "                        \"ConservativeDemocrat\",\"moderatepolitics\",\"ChapoTrapHouse\"]\n",
    "right_subreddits = [\"The_Donald\",\"Conservative\",\"ShitPoliticsSays\",\"progun\",\"Republican\",\"Capitalism\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim Reduction -> 2 dimensions\n",
    "pca =  PCA(n_components = 2)\n",
    "two_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "two_dim[[\"subreddit\",\"week\"]] = sub_df\n",
    "idx = pd.MultiIndex.from_product([two_dim['week'].unique(), two_dim['subreddit'].unique()],\n",
    "                                 names=['week', 'subreddit'])\n",
    "\n",
    "# In the case that there isn't a vector for a specific week/subreddit we bacfill the vector from the previous\n",
    "two_dim = two_dim.set_index(['week', 'subreddit']).reindex(idx).reset_index().sort_values('week').bfill()\n",
    "two_dim = two_dim[two_dim[\"subreddit\"].isin(left_subreddits) | two_dim[\"subreddit\"].isin(right_subreddits)]\n",
    "two_dim[\"partisan\"] = np.where(two_dim[\"subreddit\"].isin(left_subreddits), 'left', 'right')\n",
    "two_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'two_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1cee9a2c0f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m args = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"hover_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'two_dim' is not defined"
     ]
    }
   ],
   "source": [
    "(max_x, max_y), (min_x, min_y) = two_dim[[0,1]].max(axis=0), two_dim[[0,1]].min(axis=0)\n",
    "args = {\n",
    "    \"x\": 0,\n",
    "    \"y\": 1,\n",
    "    \"hover_name\": \"subreddit\",\n",
    "    \"text\": \"subreddit\",\n",
    "    \"opacity\": 0.7,\n",
    "    \"color\": \"partisan\",\n",
    "    \"animation_frame\": two_dim.week.astype(str),\n",
    "    \"animation_group\": \"subreddit\",\n",
    "    \"range_x\": [min_x-3,max_x+3],\n",
    "    \"range_y\": [min_y-3,min_y+3],\n",
    "}\n",
    "fig = px.scatter(two_dim,**args)\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.write_html(\"visualizations/temporal/{}_2d_scatter.html\".format(TIME_FRAME))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3c9c0cc58e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PCA Dim Reduction -> 3 dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthree_dim\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthree_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m idx = pd.MultiIndex.from_product([three_dim['week'].unique(), three_dim['subreddit'].unique()],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# PCA Dim Reduction -> 3 dimensions\n",
    "pca =  PCA(n_components = 3)\n",
    "three_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "three_dim[[\"subreddit\",\"week\"]] = sub_df\n",
    "idx = pd.MultiIndex.from_product([three_dim['week'].unique(), three_dim['subreddit'].unique()],\n",
    "                                 names=['week', 'subreddit'])\n",
    "\n",
    "# In the case that there isn't a vector for a specific week/subreddit we bacfill the vector from the previous\n",
    "three_dim = three_dim.set_index(['week', 'subreddit']).reindex(idx).reset_index().sort_values('week').bfill()\n",
    "three_dim = three_dim[three_dim[\"subreddit\"].isin(left_subreddits) | three_dim[\"subreddit\"].isin(right_subreddits)]\n",
    "three_dim[\"partisan\"] = np.where(three_dim[\"subreddit\"].isin(left_subreddits), 'left', 'right')\n",
    "three_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(max_x, max_y, max_z), (min_x, min_y, min_z) = three_dim[[0,1,2]].max(axis=0), three_dim[[0,1,2]].min(axis=0)\n",
    "args = {\n",
    "    \"x\": 0,\n",
    "    \"y\": 1,\n",
    "    \"z\": 2,\n",
    "    \"hover_name\": \"subreddit\",\n",
    "#     \"text\": \"subreddit\",\n",
    "    \"opacity\": 0.7,\n",
    "    \"color\": \"partisan\",\n",
    "    \"animation_frame\": three_dim.week.astype(str),\n",
    "    \"animation_group\": \"subreddit\",\n",
    "    \"range_x\": [min_x-3,max_x+3],\n",
    "    \"range_y\": [min_y-3,min_y+3],\n",
    "    \"range_z\": [min_z-3,max_z+1]\n",
    "\n",
    "}\n",
    "fig = px.scatter_3d(three_dim,**args)\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.write_html(\"visualizations/temporal/{}_3d_scatter.html\".format(TIME_FRAME))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
