{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/h/224/cameron/spark-3.0.0-preview2-bin-hadoop2.7\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_sub\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()\n",
    "TIME_FRAME = \"weekly\"\n",
    "window = {\n",
    "    \"weekly\": 7,\n",
    "    \"biweekly\": 14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = false)\n",
      " |-- subreddit: string (nullable = false)\n",
      " |-- created_utc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Parquet data\n",
    "comments = spark.read.load(\"/comments_2019.parquet\").fillna(\"\")\n",
    "subreddits = spark.read.load(\"dataframes/subreddits.parquet\")\n",
    "comments = comments.join(subreddits, ['subreddit'], 'leftsemi')\n",
    "cols = ['author','subreddit','created_utc']\n",
    "comments = comments.select(*cols)\n",
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              author|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|         andrewmyles|(NintendoSwitch,2...|\n",
      "|             SeeDeez| (nyjets,2019-02-17)|\n",
      "|       Serious_Sam_2|(RocketLeagueExch...|\n",
      "|           [deleted]|(worldnews,2019-0...|\n",
      "|      RanietsSharvas|(AnthemTheGame,20...|\n",
      "|            Nole2424|(apexlegends,2019...|\n",
      "|         mrfuckhead1|(synthesizers,201...|\n",
      "|       thelucidvegan| (movies,2019-02-17)|\n",
      "|IShouldWashTheDishes|(apexlegends,2019...|\n",
      "|           tardis_11|(apexlegends,2019...|\n",
      "|        Hamburtle666|(relationships,20...|\n",
      "|     browneyedgirl79|(Random_Acts_Of_A...|\n",
      "|           TumNarDok|(politics,2019-02...|\n",
      "|   mywaterlooaccount|  (memes,2019-02-17)|\n",
      "|        winterman666|(todayilearned,20...|\n",
      "|disposableaccountass|(politics,2019-02...|\n",
      "|             VM_1701|(MortalKombat,201...|\n",
      "|       brave_pumpkin|(KotakuInAction,2...|\n",
      "|           JibberGXP|(FortNiteBR,2019-...|\n",
      "|            Reisz618|(AskReddit,2019-0...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add date column\n",
    "# Create a function that returns the desired UDF from a timestamp \n",
    "to_udf = udf(lambda ts: datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:00:00\"))\n",
    "\n",
    "comments = comments.withColumn(\"timestamp\", to_udf(comments[\"created_utc\"]))\n",
    "# Add column that aggregates by week \n",
    "comments = comments.withColumn(\"week\",date_sub(next_day(col(\"timestamp\"),\"sunday\"),window[TIME_FRAME]))\n",
    "comments = comments.withColumn('subreddit', concat(lit('('),col('subreddit'),lit(','),col('week'),lit(')')))\n",
    "comments = comments.drop(*[\"created_utc\",\"timestamp\",\"week\"])\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecf Files\n",
    "[Word2vecf](https://github.com/BIU-NLP/word2vecf/blob/master/README.md) requires three inputs\n",
    "* training_data: text file of word-context pairs (space delimited)\n",
    "* word_vocabulary: file mapping subreddits (strings) to their counts\n",
    "* count_vocabulary: file mapping users (contexts -> subreddit commenters) to their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "We want to avoid having to load the raw data as there are 1 billion+ rows. Working with aggregates from the start makes things much easier. \n",
    "\n",
    "*Since this is the temporal embedding we've already added the week into the subreddit name. Each subreddit/week combo is considered a new word with possible different contexts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|           subreddit|            author|count|\n",
      "+--------------------+------------------+-----+\n",
      "|  (memes,2019-02-17)| mywaterlooaccount|    1|\n",
      "|(AnthemTheGame,20...|      Snow56border|   15|\n",
      "|(MortalKombat,201...|      Frandaman760|   29|\n",
      "|(AskReddit,2019-0...|          stefan5b|   97|\n",
      "| (sports,2019-02-17)|         roaragami|    1|\n",
      "|(AskReddit,2019-0...|    sweetbabygreen|   29|\n",
      "|(intermittentfast...|        frevernewb|   15|\n",
      "| (NHLHUT,2019-02-17)|      BX_Islanders|  105|\n",
      "|(melbourne,2019-0...|      Sys6473eight|   37|\n",
      "|(relationship_adv...|        _____i____|   40|\n",
      "|    (nba,2019-02-17)|         EandT1003|    3|\n",
      "|(JordanPeterson,2...|humanoidxincognito|    2|\n",
      "|(specializedtools...|    MuddyScroll360|    1|\n",
      "|(entitledparents,...|     AutoModerator| 3833|\n",
      "|(starcraft,2019-0...|         [deleted]|  489|\n",
      "|(The_Donald,2019-...|   mimefortheblind|    9|\n",
      "|  (memes,2019-02-17)|         Daneiiiil|    1|\n",
      "|(chicago,2019-02-17)|villagethriftidiot|   14|\n",
      "|(AskOuija,2019-02...|    BuoyantArmiger|   33|\n",
      "| (hockey,2019-02-17)|     WingerSupreme|  125|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = comments.groupBy([\"subreddit\",\"author\"]).count().cache()\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301431081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "word_vocabulary = training_data.groupBy(\"subreddit\").agg(_sum('count').alias('count')).cache()\n",
    "word_vocabulary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocabulary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              author| count|\n",
      "+--------------------+------+\n",
      "|   sillygaythrowaway|   197|\n",
      "|             Faulkal|    99|\n",
      "|        Tittypookaka|   753|\n",
      "|       DankMemesMods|207100|\n",
      "|        aeonianvibes|   116|\n",
      "|                 n67|   254|\n",
      "|     Jason_Whorehees|  1413|\n",
      "|     bookmovietvworm|  1991|\n",
      "|         andreagassi|   604|\n",
      "|        Beebeemybaby|   248|\n",
      "|WestCoastBestCoast01|  1826|\n",
      "|    NipDrunkChipmunk|    24|\n",
      "|     angrykeyboarder|  1098|\n",
      "|     Nameless_king69|   258|\n",
      "|            nuhtalia|    55|\n",
      "|          RhysRuther|    14|\n",
      "|       Spocks-Nephew|  4064|\n",
      "|              nshane|   455|\n",
      "|        SansasAgency|   512|\n",
      "|           Minic3211|   303|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vocabulary = training_data.groupBy(\"author\").agg(_sum('count').alias('count')).cache()\n",
    "context_vocabulary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15283648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocabulary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Vocabularies and Training Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='mkdir -p /h/224/cameron/Political-Subreddit-Embedding/temp/temporal/', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temp context for the word and context vocabulary files (which get passed to the word2vecf script)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "temp_dir = \"/h/224/cameron/Political-Subreddit-Embedding/temp/temporal/\"\n",
    "subprocess.run(\"mkdir -p {}\".format(temp_dir), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp files\n",
    "file_data = os.path.join(temp_dir, '{}_data.txt'.format(TIME_FRAME))\n",
    "file_wv = os.path.join(temp_dir, '{}_wv.txt'.format(TIME_FRAME))\n",
    "file_cv = os.path.join(temp_dir, '{}_cv.txt'.format(TIME_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing training data to {}...\".format(file_data))\n",
    "training_data.toPandas().to_csv(file_data, header=False, index=False, sep=' ')\n",
    "training_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing word vocab data to {}...\".format(file_wv))\n",
    "word_vocabulary.toPandas().to_csv(file_wv, header=False, index=False, sep=' ')\n",
    "word_vocabulary.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing context vocab data to {}...\".format(file_cv))\n",
    "context_vocabulary.toPandas().to_csv(file_cv, header=False, index=False, sep=' ')\n",
    "context_vocabulary.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters, using negative sampling\n",
    "# -alpha 0.18 -negative 35 -sample 0.0043 -size 150\n",
    "from utils import generate_embedding, load_embedding\n",
    "embedding_args = {\n",
    "                    \"param1\": \"sample\", \n",
    "                    \"p1\": 0.0043, \n",
    "                    \"param2\": \"negative\", \n",
    "                    \"p2\": 35, \n",
    "                    \"file_data\": file_data , \n",
    "                    \"file_wv\": file_wv, \n",
    "                    \"file_cv\": file_cv,\n",
    "                    \"size\": 150,\n",
    "                    \"alpha\": 0.18\n",
    "                 }\n",
    "embedding = generate_embedding(embedding_args)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits, vectors = load_embedding(embedding)\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Out Subreddit from Week Again\n",
    "\n",
    "Since we've already trained all of the seperate emebeddings there isn't a need for them to be in the same column anymore. This will make animating the emebedding over time easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_tup\n",
    "\n",
    "sub_df = pd.DataFrame(subreddits.apply(parse_tup).tolist())\n",
    "sub_df.columns = [\"subreddit\",\"week\"]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize/Animate\n",
    "1. Reduce to 3/2 dimensions\n",
    "2. Add subreddit/week columns to factored dataframe\n",
    "3. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "subprocess.run(\"mkdir -p visualizations/temporal\", shell=True)\n",
    "left_subreddits = [\"JoeBiden\",\"Pete_Buttigieg\",\"Kamala\",\n",
    "                        \"SandersForPresident\",\"BetoORourke\",\"ElizabethWarren\",\n",
    "                        \"BaemyKlobaechar\",\"YangForPresidentHQ\",\"politics\",\"progressive\",\n",
    "                        \"demsocialist\",\"SocialDemocracy\",\"centerleftpolitics\",\n",
    "                        \"ConservativeDemocrat\",\"moderatepolitics\",\"ChapoTrapHouse\"]\n",
    "right_subreddits = [\"The_Donald\",\"Conservative\",\"ShitPoliticsSays\",\"progun\",\"Republican\",\"Capitalism\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim Reduction -> 2 dimensions\n",
    "pca =  PCA(n_components = 2)\n",
    "two_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "two_dim[[\"subreddit\",\"week\"]] = sub_df\n",
    "idx = pd.MultiIndex.from_product([two_dim['week'].unique(), two_dim['subreddit'].unique()],\n",
    "                                 names=['week', 'subreddit'])\n",
    "\n",
    "# In the case that there isn't a vector for a specific week/subreddit we bacfill the vector from the previous\n",
    "two_dim = two_dim.set_index(['week', 'subreddit']).reindex(idx).reset_index().sort_values('week').bfill()\n",
    "two_dim = two_dim[two_dim[\"subreddit\"].isin(left_subreddits) | two_dim[\"subreddit\"].isin(right_subreddits)]\n",
    "two_dim[\"partisan\"] = np.where(two_dim[\"subreddit\"].isin(left_subreddits), 'left', 'right')\n",
    "two_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'two_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1cee9a2c0f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m args = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"hover_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'two_dim' is not defined"
     ]
    }
   ],
   "source": [
    "(max_x, max_y), (min_x, min_y) = two_dim[[0,1]].max(axis=0), two_dim[[0,1]].min(axis=0)\n",
    "args = {\n",
    "    \"x\": 0,\n",
    "    \"y\": 1,\n",
    "    \"hover_name\": \"subreddit\",\n",
    "    \"text\": \"subreddit\",\n",
    "    \"opacity\": 0.7,\n",
    "    \"color\": \"partisan\",\n",
    "    \"animation_frame\": two_dim.week.astype(str),\n",
    "    \"animation_group\": \"subreddit\",\n",
    "    \"range_x\": [min_x-3,max_x+3],\n",
    "    \"range_y\": [min_y-3,min_y+3],\n",
    "}\n",
    "fig = px.scatter(two_dim,**args)\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.write_html(\"visualizations/temporal/{}_2d_scatter.html\".format(TIME_FRAME))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3c9c0cc58e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PCA Dim Reduction -> 3 dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthree_dim\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthree_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m idx = pd.MultiIndex.from_product([three_dim['week'].unique(), three_dim['subreddit'].unique()],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# PCA Dim Reduction -> 3 dimensions\n",
    "pca =  PCA(n_components = 3)\n",
    "three_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "three_dim[[\"subreddit\",\"week\"]] = sub_df\n",
    "idx = pd.MultiIndex.from_product([three_dim['week'].unique(), three_dim['subreddit'].unique()],\n",
    "                                 names=['week', 'subreddit'])\n",
    "\n",
    "# In the case that there isn't a vector for a specific week/subreddit we bacfill the vector from the previous\n",
    "three_dim = three_dim.set_index(['week', 'subreddit']).reindex(idx).reset_index().sort_values('week').bfill()\n",
    "three_dim = three_dim[three_dim[\"subreddit\"].isin(left_subreddits) | three_dim[\"subreddit\"].isin(right_subreddits)]\n",
    "three_dim[\"partisan\"] = np.where(three_dim[\"subreddit\"].isin(left_subreddits), 'left', 'right')\n",
    "three_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(max_x, max_y, max_z), (min_x, min_y, min_z) = three_dim[[0,1,2]].max(axis=0), three_dim[[0,1,2]].min(axis=0)\n",
    "args = {\n",
    "    \"x\": 0,\n",
    "    \"y\": 1,\n",
    "    \"z\": 2,\n",
    "    \"hover_name\": \"subreddit\",\n",
    "#     \"text\": \"subreddit\",\n",
    "    \"opacity\": 0.7,\n",
    "    \"color\": \"partisan\",\n",
    "    \"animation_frame\": three_dim.week.astype(str),\n",
    "    \"animation_group\": \"subreddit\",\n",
    "    \"range_x\": [min_x-3,max_x+3],\n",
    "    \"range_y\": [min_y-3,min_y+3],\n",
    "    \"range_z\": [min_z-3,max_z+1]\n",
    "\n",
    "}\n",
    "fig = px.scatter_3d(three_dim,**args)\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.write_html(\"visualizations/temporal/{}_3d_scatter.html\".format(TIME_FRAME))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
