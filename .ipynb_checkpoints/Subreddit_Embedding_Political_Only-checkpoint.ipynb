{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/h/224/cameron/spark-3.0.0-preview2-bin-hadoop2.7\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'adavm1.ais.sandbox'),\n",
       " ('spark.driver.memory', '400g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.app.id', 'local-1590801054061'),\n",
       " ('spark.executor.memory', '400g'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '45557'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.cores', '16'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = false)\n",
      " |-- author: string (nullable = false)\n",
      " |-- created_utc: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1663587081"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Parquet data\n",
    "# df = spark.read.load(\"/comments_2019.parquet\").fillna(\"\")\n",
    "# df.createOrReplaceTempView(\"comments\")\n",
    "# political_comments = spark.sql(\"\"\"select * from comments \n",
    "#                                   where subreddit in \n",
    "#                                       (\"JoeBiden\",\"Pete_Buttigieg\",\"Kamala\",\n",
    "#                                       \"SandersForPresident\",\"BetoORourke\",\"ElizabethWarren\",\n",
    "#                                       \"BaemyKlobaechar\",\"YangForPresidentHQ\",\"politics\",\"progressive\",\n",
    "#                                       \"demsocialist\",\"SocialDemocracy\",\"centerleftpolitics\",\"ConservativeDemocrat\",\n",
    "#                                       \"moderatepolitics\")\n",
    "#                                 \"\"\")\n",
    "political_comments = spark.read.load(\"/comments_2019.parquet\").fillna(\"\")\n",
    "# political_comments.createOrReplaceTempView(\"comments\")\n",
    "political_comments = political_comments.select(\"subreddit\", \"author\", \"created_utc\")\n",
    "political_comments.printSchema()\n",
    "political_comments.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vecf Files\n",
    "[Word2vecf](https://github.com/BIU-NLP/word2vecf/blob/master/README.md) requires three inputs\n",
    "* word_vocabulary: file mapping subreddits (strings) to their counts\n",
    "* count_vocabulary: file mapping users (contexts -> subreddit commenters) to their counts\n",
    "* training_data: text file of word-context pairs (space delimited)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|          subreddit|  count|\n",
      "+-------------------+-------+\n",
      "|              anime|3303822|\n",
      "|       gentlefemdom|  62808|\n",
      "|           Goldfish|  27526|\n",
      "|         MLBTheShow| 587781|\n",
      "|             travel| 386688|\n",
      "|         costa_rica|  11099|\n",
      "|       SaltLakeCity| 125313|\n",
      "|UnresolvedMysteries| 332677|\n",
      "|     TrueOffMyChest| 726501|\n",
      "|         traderjoes|  69021|\n",
      "|             AdPorn|   3857|\n",
      "|         MensRights| 503714|\n",
      "|            Amateur| 114799|\n",
      "|  BeautyGuruChatter| 564832|\n",
      "|         NHLStreams|  67022|\n",
      "|          GemsofWar|  11396|\n",
      "|      gastricsleeve|  31746|\n",
      "|                NIU|   3486|\n",
      "|    NewLondonCounty|  22403|\n",
      "|    Notakeonlythrow|   6424|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_vocabulary = political_comments.groupBy(\"subreddit\").count()\n",
    "word_vocabulary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocabulary = political_comments.groupBy(\"author\").count()\n",
    "context_vocabulary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = political_comments.groupBy(\"subreddit\",\"author\").count()\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = word_vocabulary.toPandas()\n",
    "fig = plt.figure()\n",
    "plt.yscale('log')\n",
    "plt.title('Number Comments per Political Subreddit')\n",
    "\n",
    "# plt.xlabel('Duration (in seconds)')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.bar(wdf[\"subreddit\"],wdf[\"count\"])\n",
    "plt.xticks(rotation='vertical')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp context for the word and context vocabulary files (which get passed to the word2vecf script)\n",
    "temp_dir = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp files\n",
    "file_data = os.path.join(temp_dir.name, 'data.txt')\n",
    "file_wv = os.path.join(temp_dir.name, 'wv.txt')\n",
    "file_cv = os.path.join(temp_dir.name, 'cv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing training data to {}...\".format(file_data))\n",
    "training_data.select(\"subreddit\", \"author\").toPandas().to_csv(file_data, header=False, index=False, sep=' ')\n",
    "print(\"Writing word vocab data to {}...\".format(file_wv))\n",
    "word_vocabulary.toPandas().to_csv(file_wv, header=False, index=False, sep=' ')\n",
    "print(\"Writing context vocab data to {}...\".format(file_cv))\n",
    "context_vocabulary.toPandas().to_csv(file_cv, header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters, using negative sampling\n",
    "sample = \"sample\"\n",
    "lr = 0.0082\n",
    "# Training algorithm: hierarchical softmax or negative sampling\n",
    "training_alg = \"negative\"\n",
    "negative = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def generate_embedding(p1, p2, param1, param2, file_data, file_wv, file_cv):\n",
    "    output = \"vecs_{}_{}.txt\".format(p1,p2)\n",
    "    command = \"./word2vecf/word2vecf -train {} -wvocab {} -cvocab {} -output {} -threads 180 -alpha 0.26 -size 200 -{} {} -{} {}\".format(file_data,file_wv,file_cv,output,param1,p1,param2,p2)\n",
    "    if not os.path.exists(output):\n",
    "        print(command)\n",
    "        subprocess.run(command, shell=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_args = {\"param1\": sample, \n",
    "                  \"p1\": lr, \n",
    "                  \"param2\": training_alg, \n",
    "                  \"p2\": negative, \n",
    "                  \"file_data\": file_data , \n",
    "                  \"file_wv\": file_wv, \n",
    "                  \"file_cv\": file_cv\n",
    "                 }\n",
    "generated_embeddings = generate_embedding(**embedding_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    embedding = pd.read_csv(filename, sep=' ', header=None, skiprows=1)\n",
    "    embedding.set_index(0)\n",
    "    embedding = embedding.rename(columns={0: 'subreddits'})\n",
    "    subreddits, vectors = embedding.iloc[:, 0], embedding.iloc[:, 1:200]\n",
    "    vectors = vectors.divide(np.linalg.norm(vectors, axis=1), axis=0)\n",
    "    return subreddits, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits, vectors = load_embedding(generated_embeddings)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to 3 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# PCA Dim Reduction\n",
    "pca =  PCA(n_components = 3)\n",
    "three_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "three_dim['subreddit'] = subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(three_dim, x=0, y=1, z=2,text=\"subreddit\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim Reduction\n",
    "pca =  PCA(n_components = 2)\n",
    "two_dim =  pd.DataFrame(pca.fit_transform(vectors))\n",
    "two_dim['subreddit'] = subreddits\n",
    "fig = px.scatter(two_dim, x=0, y=1,text=\"subreddit\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
